{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIA_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VGbrw2B2fFHC",
        "KKkNBovufqiy",
        "6wxfafWUfJoB",
        "F9yH5efYi-k6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGbrw2B2fFHC"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Z49apdiZeszU",
        "outputId": "4f093031-b1fd-4bd4-ec2f-acb8917511be"
      },
      "source": [
        "!pip install itk\n",
        "#import SimpleITK as sitk\n",
        "import numpy as np\n",
        "import os \n",
        "import itk\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import glob\n",
        "from os import listdir\n",
        "import glob\n",
        "import tqdm\n",
        "from typing import Dict\n",
        "import cv2\n",
        "import pydicom as dicom\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import iplot\n",
        "import plotly.figure_factory as ff\n",
        "import cufflinks\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder,PowerTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score,cross_validate, KFold,GroupKFold\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: itk in /usr/local/lib/python3.6/dist-packages (5.1.1.post1)\n",
            "Requirement already satisfied: itk-segmentation==5.1.1.post1 in /usr/local/lib/python3.6/dist-packages (from itk) (5.1.1.post1)\n",
            "Requirement already satisfied: itk-io==5.1.1.post1 in /usr/local/lib/python3.6/dist-packages (from itk) (5.1.1.post1)\n",
            "Requirement already satisfied: itk-numerics==5.1.1.post1 in /usr/local/lib/python3.6/dist-packages (from itk) (5.1.1.post1)\n",
            "Requirement already satisfied: itk-core==5.1.1.post1 in /usr/local/lib/python3.6/dist-packages (from itk) (5.1.1.post1)\n",
            "Requirement already satisfied: itk-filtering==5.1.1.post1 in /usr/local/lib/python3.6/dist-packages (from itk) (5.1.1.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from itk) (1.18.5)\n",
            "Requirement already satisfied: itk-registration==5.1.1.post1 in /usr/local/lib/python3.6/dist-packages (from itk) (5.1.1.post1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d10e6ffe5a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydicom\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdicom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydicom'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKkNBovufqiy"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "Either use API command: kaggle competitions download -c osic-pulmonary-fibrosis-progression\n",
        "or obtain data from here: https://www.osicild.org/kaggle-challenge.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HJVwzXDgFrp"
      },
      "source": [
        "train = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/train.csv\")\n",
        "test = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/test.csv\")\n",
        "train_data = train\n",
        "test_data = test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9_wVJ5_gISp"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGC_Vx0jgKap"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wxfafWUfJoB"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trmAMWCKfEhR"
      },
      "source": [
        "#Figure 1 Generate FVC distribution\n",
        "sns.distplot(train[\"FVC\"],color=\"blue\")\n",
        "plt.title(\"FVC Data Distribution\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IskYuABghNGZ"
      },
      "source": [
        "#Figure 2 FVC distribution across sex\n",
        "for i in train.Sex.unique():\n",
        "        sns.distplot(train[train['Sex']==i][\"FVC\"],label=i)\n",
        "plt.title(\"FVC Scores Per Sex\");\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3-mvTnah9mx"
      },
      "source": [
        "#Figure 3 Smoking Status across sex\n",
        "sns.countplot(data=train,x='SmokingStatus',hue='Sex');\n",
        "plt.title(\"Smoking Status across Sex\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX5Zbyg7iioS"
      },
      "source": [
        "#Figure 4 Using Plotly to show FVC distrubution across sex and smoking status\n",
        "px.box(train,x=\"Sex\",y=\"FVC\",color=\"SmokingStatus\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9yH5efYi-k6"
      },
      "source": [
        "# Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTELVnKMiqNR"
      },
      "source": [
        "train_images = os.listdir('../input/osic-pulmonary-fibrosis-progression/train/') #Change to correct directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDZF1uA0i8CL"
      },
      "source": [
        "#Sample DICOM image using pydicom\n",
        "image = train_image_path+train_images[0]+\"/1.dcm\"\n",
        "image = dicom.dcmread(image)\n",
        "image = image.pixel_array\n",
        "plt.imshow(image,cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnrFHw_GktKr"
      },
      "source": [
        "#Cool Animation Display, function from: https://github.com/pydicom/pydicom/blob/master/examples/image_processing/reslice.py (Credit to: https://www.kaggle.com/maunish)\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "def show_animation():\n",
        "    rn = np.random.randint(0,len(train_images),1)[0]\n",
        "    fig = plt.figure()\n",
        "    path= train_image_path+train_images[0]\n",
        "    images = [dicom.read_file(path+\"/\"+img) for img in os.listdir(path)]\n",
        "    images.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n",
        "    ims = list()\n",
        "    for image in images:\n",
        "        image = plt.imshow(image.pixel_array,cmap='gray',animated=True)\n",
        "        plt.axis('off')\n",
        "        ims.append([image])\n",
        "    ani = animation.ArtistAnimation(fig,ims,interval=100,blit=False,repeat_delay=1000)\n",
        "    return ani\n",
        "ani = show_animation()\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLx5xEYRlccU"
      },
      "source": [
        "folder_path = '../input/osic-pulmonary-fibrosis-progression'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TogDxY1SoovE"
      },
      "source": [
        "#Helper functions from https://www.kaggle.com/maunish\n",
        "#getting base week for patient\n",
        "def get_baseline_week(data):\n",
        "    df = data.copy()\n",
        "    df['Weeks'] = df['Weeks'].astype(int)\n",
        "    df['min_week'] = df.groupby('Patient')['Weeks'].transform('min')\n",
        "    df['baseline_week'] = df['Weeks'] - df['min_week']\n",
        "    return df\n",
        "\n",
        "#getting FVC for base week and setting it as base_FVC of patient\n",
        "def get_base_FVC(data):\n",
        "    df = data.copy()\n",
        "    base = df.loc[df.Weeks == df.min_week][['Patient','FVC']].copy()\n",
        "    base.columns = ['Patient','base_FVC']\n",
        "    \n",
        "    base['nb']=1\n",
        "    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n",
        "    \n",
        "    base = base[base.nb==1]\n",
        "    base.drop('nb',axis =1,inplace=True)\n",
        "    df = df.merge(base,on=\"Patient\",how='left')\n",
        "    df.drop(['min_week'], axis = 1)\n",
        "    return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdlM6R5po5iL"
      },
      "source": [
        "train_data = get_baseline_week(train_data)\n",
        "train_data = get_base_FVC(train_data)\n",
        "train_columns = ['baseline_week','base_FVC','Percent','Age','Sex','SmokingStatus']\n",
        "train_label = ['FVC']\n",
        "train = train_data[train_columns]\n",
        "\n",
        "#Preprocess (Z-normalize + 1-Hot encode feature labels)\n",
        "transformer = ColumnTransformer([('s',StandardScaler(),[0,1,2,3]),('o',OneHotEncoder(),[4,5])])\n",
        "target = train_data[train_label].values\n",
        "train = transformer.fit_transform(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lghfE_XoPSF"
      },
      "source": [
        "# Building PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pN2SYv4peQr"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,n):\n",
        "        super(Model,self).__init__()\n",
        "        self.layer1 = nn.Linear(n,200)\n",
        "        self.layer2 = nn.Linear(200,100)\n",
        "        \n",
        "        self.out1 = nn.Linear(100,3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.out2 = nn.Linear(100,3)\n",
        "            \n",
        "    def forward(self,xb):\n",
        "        x1 =  F.leaky_relu(self.layer1(xb))\n",
        "        x1 =  F.leaky_relu(self.layer2(x1))\n",
        "        \n",
        "        o1 = self.out1(x1)\n",
        "        o2 = F.relu(self.out2(x1))\n",
        "        return o1 + torch.cumsum(o2,dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOW8y1aUuVF3"
      },
      "source": [
        "#run the model, note only 1 fold used in actual test, very slow using cpu\n",
        "def run():\n",
        "    \n",
        "    def score(outputs,target):\n",
        "        confidence = outputs[:,2] - outputs[:,0]\n",
        "        clip = torch.clamp(confidence,min=70)\n",
        "        target=torch.reshape(target,outputs[:,1].shape)\n",
        "        delta = torch.abs(outputs[:,1] - target)\n",
        "        delta = torch.clamp(delta,max=1000)\n",
        "        sqrt_2 = torch.sqrt(torch.tensor([2.])).to(device)\n",
        "        metrics = (delta*sqrt_2/clip) + torch.log(clip*sqrt_2)\n",
        "        return torch.mean(metrics)\n",
        "    \n",
        "    def qloss(outputs,target):\n",
        "        qs = [0.25,0.5,0.75]\n",
        "        qs = torch.tensor(qs,dtype=torch.float).to(device)\n",
        "        e =  target - outputs\n",
        "        e.to(device)\n",
        "        v = torch.max(qs*e,(qs-1)*e)\n",
        "        v = torch.sum(v,dim=1)\n",
        "        return torch.mean(v)\n",
        "    \n",
        "    def loss_fn(outputs,target,l):\n",
        "        return l * qloss(outputs,target) + (1- l) * score(outputs,target)\n",
        "        \n",
        "    def train_loop(train_loader,model,loss_fn,device,optimizer,lr_scheduler=None):\n",
        "        model.train()\n",
        "        losses = list()\n",
        "        metrics = list()\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(True):           \n",
        "                outputs = model(inputs)                 \n",
        "                metric = score(outputs,labels)\n",
        "\n",
        "                loss = loss_fn(outputs,labels,0.8)\n",
        "                metrics.append(metric.cpu().detach().numpy())\n",
        "                losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                if lr_scheduler != None:\n",
        "                    lr_scheduler.step()\n",
        "            \n",
        "        return losses,metrics\n",
        "    \n",
        "    def valid_loop(valid_loader,model,loss_fn,device):\n",
        "        model.eval()\n",
        "        losses = list()\n",
        "        metrics = list()\n",
        "        for i, (inputs, labels) in enumerate(valid_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)                 \n",
        "            metric = score(outputs,labels)\n",
        "            \n",
        "            loss = loss_fn(outputs,labels,0.8)\n",
        "            metrics.append(metric.cpu().detach().numpy())\n",
        "            losses.append(loss.cpu().detach().numpy())\n",
        "            \n",
        "        return losses,metrics    \n",
        "\n",
        "    NFOLDS =5\n",
        "    kfold = KFold(NFOLDS,shuffle=True,random_state=42)\n",
        "    \n",
        "    #kfold\n",
        "    for k , (train_idx,valid_idx) in enumerate(kfold.split(train)):\n",
        "        batch_size = 64\n",
        "        epochs = 50\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"{device} is used\")\n",
        "        x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx],target[valid_idx]\n",
        "        n = x_train.shape[1]\n",
        "        model = Model(n)\n",
        "        model.to(device)\n",
        "        lr = 0.1\n",
        "        optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "        train_tensor = torch.tensor(x_train,dtype=torch.float)\n",
        "        y_train_tensor = torch.tensor(y_train,dtype=torch.float)\n",
        "\n",
        "        train_ds = TensorDataset(train_tensor,y_train_tensor)\n",
        "        train_dl = DataLoader(train_ds,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers=4,\n",
        "                             shuffle=True\n",
        "                             )\n",
        "\n",
        "        valid_tensor = torch.tensor(x_valid,dtype=torch.float)\n",
        "        y_valid_tensor = torch.tensor(y_valid,dtype=torch.float)\n",
        "\n",
        "        valid_ds = TensorDataset(valid_tensor,y_valid_tensor)\n",
        "        valid_dl = DataLoader(valid_ds,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers=4,\n",
        "                             shuffle=False\n",
        "                             )\n",
        "        \n",
        "        print(f\"Fold {k}\")\n",
        "        for i in range(epochs):\n",
        "            losses,metrics = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler)\n",
        "            valid_losses,valid_metrics = valid_loop(valid_dl,model,loss_fn,device)\n",
        "            if (i+1)%5==0:\n",
        "                print(f\"epoch:{i} Training | loss:{np.mean(losses)} score: {np.mean(metrics)}| \\n Validation | loss:{np.mean(valid_losses)} score:{np.mean(valid_metrics)}|\")\n",
        "        torch.save(model.state_dict(),f'model{k}.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1mEFWFuXYd0"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om4LPBp4XYMU"
      },
      "source": [
        "def inference():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    nfold = 5\n",
        "    all_prediction = np.zeros((test.shape[0],3))\n",
        "    \n",
        "    for i in range(nfold):\n",
        "        n = train.shape[1]\n",
        "        \n",
        "        model = Model(n)\n",
        "        model.load_state_dict(torch.load(f\"model{i}.bin\"))\n",
        "        predictions = list()\n",
        "        model.to(device)\n",
        "        test_tensor = torch.tensor(test,dtype=torch.float)\n",
        "        test_dl = DataLoader(test_tensor,\n",
        "                        batch_size=64,\n",
        "                        num_workers=2,\n",
        "                        shuffle=False)\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            for i, inputs in enumerate(test_dl):\n",
        "                inputs = inputs.to(device, dtype=torch.float)\n",
        "                outputs= model(inputs) \n",
        "                predictions.extend(outputs.cpu().detach().numpy())\n",
        "\n",
        "        all_prediction += np.array(predictions)/nfold\n",
        "        \n",
        "    return all_prediction  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YITmMqMFzk77"
      },
      "source": [
        "prediction = inference()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}